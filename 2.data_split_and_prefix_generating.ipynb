{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "import copy\n",
    "import pickle\n",
    "import os \n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=\"cm2.5k\"\n",
    "window_size=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(datasets,window_size):\n",
    "    input_file=\"datasets/\"+datasets+\"/\"+datasets+\".csv\"\n",
    "    # window_size=100\n",
    "    data=pd.read_csv(input_file)\n",
    "    try:\n",
    "        data.columns = [\"CaseID\", \"Activity\", \"TYPE\", \"Timestamp\",\"amount\"]\n",
    "    except:\n",
    "        data.columns = [\"CaseID\",\"Activity\",\"Type\",\"Timestamp\"]\n",
    "    data.fillna(1,inplace=True)\n",
    "    listofactivity=data[\"Activity\"].unique().tolist()\n",
    "    cont_trace = data['CaseID'].value_counts(dropna=False)\n",
    "    max_trace=max(cont_trace)\n",
    "    # group by activity, resource and timestamp by caseid\n",
    "    act = data.groupby('CaseID', sort=False).agg({'Activity': lambda x: list(x)})\n",
    "    temp = data.groupby('CaseID', sort=False).agg({'Timestamp': lambda x: list(x)})\n",
    "    caseid=data.groupby(\"CaseID\",sort=False).agg({\"CaseID\":lambda x:list(x)})\n",
    "    # if not os.path.exists(\"datasets/\"+datasets+\"/\"+datasets+\"_\"+str(window_size)):\n",
    "    #     os.mkdir(\"datasets/\"+datasets+\"/\"+datasets+\"_\"+str(window_size))\n",
    "    # else:\n",
    "    #     return\n",
    "    # ttime=\"2799-02-2015:57:07\"\n",
    "    # date_format_str = '%Y-%m-%d%H:%M:%S'\n",
    "    # time.strptime(ttime, date_format_str)\n",
    "    def time_format(ttime):\n",
    "        ttime=\" \".join(ttime.split(\"T\"))\n",
    "        ttime=ttime.split(\".\")[0]\n",
    "        try:\n",
    "            date_format_str = '%Y-%m-%d %H:%M:%S'\n",
    "            conversion = time.strptime(ttime, date_format_str)\n",
    "        except:\n",
    "            date_format_str = '%Y/%m/%d %H:%M:%S'\n",
    "            conversion = time.strptime(ttime, date_format_str)\n",
    "        return conversion\n",
    "    time_differences=[1e9,-1]\n",
    "    time_lasts=[1e9,-1]\n",
    "    time_remaining=[1e9,-1]\n",
    "    def prefix_padding_time_normalize(prefixs):\n",
    "        for i in range(len(prefixs)):\n",
    "            for j in range(len(prefixs[i])):\n",
    "                prefixs[i][j][2]=prefixs[i][j][2]/time_differences[1]\n",
    "        for i in range(len(prefixs)):\n",
    "            trace=prefixs[i]\n",
    "            if len(trace)<max_trace:\n",
    "                num=max_trace-len(trace)\n",
    "                for j in range(num):\n",
    "                    prefixs[i].insert(0,[0 for m in range(24+7+2+1+1)])\n",
    "        return prefixs        \n",
    "\n",
    "    def time_normalize(labels):\n",
    "        for i in range(len(labels)):\n",
    "            labels[i][1]=labels[i][1]/time_remaining[1]\n",
    "        return labels\n",
    "    def prefix_generating(sub_log):\n",
    "        act_list=sub_log[0]\n",
    "        temp_list=sub_log[1]\n",
    "        id_list=sub_log[2]\n",
    "        prefixs=[]\n",
    "        labels=[]\n",
    "        lengths=[]\n",
    "        for act_trace,temp_trace,id_trace in zip(act_list,temp_list,id_list):\n",
    "        # print(act_trace)      \n",
    "            for i in range(1,len(act_trace)):\n",
    "                act_prefix=act_trace[:i]\n",
    "                temp_prefix=temp_trace[:i]\n",
    "                id=id_trace[0]\n",
    "                tempdata=[]\n",
    "                for j in range(i):\n",
    "                    act_vector_j=np.array([id]+[act_prefix[j]])\n",
    "                    first_day=time_format(temp_trace[0])\n",
    "                    previous_day=time_format(temp_trace[j-1 if j!=0 else 0])\n",
    "                    current_day= time_format(temp_trace[j])\n",
    "                    next_day=time_format(temp_trace[i])\n",
    "                    last_day=time_format(temp_trace[-1])\n",
    "                    # temp_vector=[\"持续时间\",\"前一个时间差\",\"距离午夜时间\"]\n",
    "                    timelast=0 if j==0 else (time.mktime(current_day)-time.mktime(first_day))\n",
    "                    timedifference = 0 if j==0 else (time.mktime(current_day)-time.mktime(previous_day))\n",
    "                    timeremaining= -1*(time.mktime(current_day)-time.mktime(last_day))\n",
    "                    nexteventtime= -1*(time.mktime(current_day)-time.mktime(next_day))\n",
    "\n",
    "                    time_differences[0]=min(time_differences[0],timedifference)\n",
    "                    time_differences[1]=max(time_differences[1],timedifference)\n",
    "\n",
    "                    time_lasts[0]=min(time_lasts[0],timelast)\n",
    "                    time_lasts[1]=max(time_lasts[1],timelast)\n",
    "\n",
    "                    time_remaining[0]=min(time_remaining[0],timeremaining)\n",
    "                    time_remaining[1]=max(time_remaining[1],timeremaining)\n",
    "\n",
    "                    dayhour=np.array([1 if i==current_day.tm_hour else 0 for i in range(24)],dtype=\"float\")   \n",
    "                    wday=np.array([1 if i==current_day.tm_wday else 0 for i in range(7)],dtype=\"float\")\n",
    "                    time_vector_j = np.concatenate((np.array([timelast]),np.array([timedifference]),dayhour,wday),axis=0)\n",
    "                    # time_vector_j = np.array([timelast,timedifference,current_day.tm_hour])\n",
    "                    tempdata.append(np.concatenate((act_vector_j,time_vector_j),axis=0))\n",
    "                templabel=[act_trace[i],timeremaining]\n",
    "                prefixs.append(tempdata)\n",
    "                labels.append(templabel)\n",
    "                lengths.append(i)\n",
    "        # prefixs=prefix_padding(prefixs)\n",
    "        # labels=time_normalize(labels)\n",
    "        return prefixs,labels,lengths\n",
    "\n",
    "    act,temp=act.loc[:,\"Activity\"].values,temp.loc[:,\"Timestamp\"].values\n",
    "    caseid=caseid.loc[:,\"CaseID\"].values\n",
    "    dataset_length=np.size(act,0)\n",
    "    num_sublogs=math.ceil(dataset_length/window_size)\n",
    "    datas=[]\n",
    "    for i in range(num_sublogs):\n",
    "        start=i*window_size\n",
    "        end=min((i+1)*window_size,dataset_length)\n",
    "        result=prefix_generating([act[start:end],temp[start:end],caseid[start:end]])\n",
    "        datas.append(result)\n",
    "\n",
    "\n",
    "    for i in range(len(datas)):\n",
    "        prefixs=datas[i][0]\n",
    "        labels=datas[i][1]\n",
    "        lengths=datas[i][2]\n",
    "        prefixs=prefix_padding_time_normalize(datas[i][0])\n",
    "        labels=time_normalize(datas[i][1])\n",
    "        if not os.path.exists(\"datasets/\"+datasets+\"/\"+datasets+\"_\"+str(window_size)+\"/sub_logs_\"+str(i)):\n",
    "            os.mkdir(\"datasets/\"+datasets+\"/\"+datasets+\"_\"+str(window_size)+\"/sub_logs_\"+str(i))\n",
    "        pickle.dump(prefixs,open(\"datasets/\"+datasets+\"/\"+datasets+\"_\"+str(window_size)+\"/sub_logs_\"+str(i)+\"/\"+\"data.pkl\",\"wb\"))\n",
    "        pickle.dump(labels,open(\"datasets/\"+datasets+\"/\"+datasets+\"_\"+str(window_size)+\"/sub_logs_\"+str(i)+\"/\"+\"label.pkl\",\"wb\"))\n",
    "        pickle.dump(lengths,open(\"datasets/\"+datasets+\"/\"+datasets+\"_\"+str(window_size)+\"/sub_logs_\"+str(i)+\"/\"+\"length.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\"Base_cp10k\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "father_dir=\"datasets\"\n",
    "sub_dirs = [sub_dir for sub_dir in os.listdir(father_dir) if (os.path.isdir(os.path.join(father_dir, sub_dir)) and \"Base\" in sub_dir)]\n",
    "for sub_dir in sub_dirs:\n",
    "    generate(sub_dir,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=[\"cm2.5k\",\"cm5k\",\"cm7.5k\",\"cm10k\"]\n",
    "window_sizes=[25,50,75,100]\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(len(window_sizes)):\n",
    "        generate(datasets[i],window_sizes[j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e67272651055d1832734db27dbe5c78ae7e6195044c32c87375667fa374755b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
